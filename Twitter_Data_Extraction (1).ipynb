{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install -r \"./gdrive/MyDrive/OpenAI_climate_hackathon/requirements2.txt\""
      ],
      "metadata": {
        "id": "PiFUWiphqFyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "7G3aLxj4t8Fl"
      },
      "outputs": [],
      "source": [
        "import tweepy,json\n",
        "access_token=\"1591301586898206720-gWUxGPYmRF69skLTkmFbDBGLmCHmkB\"\n",
        "access_token_secret=\"TIleLVrPUllmLwR1Hz7GHmdN5JnqFcGOIlJRuzFkeYmh8\"\n",
        "consumer_key=\"eEib0qx8FA1FySOjvWD1vNCpT\"\n",
        "consumer_secret=\"Pf4KpxvljD2k0FjiwVyh00nuDfiGZ040jOPMzj5zm5jQ3HoloE\"\n",
        "auth= tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "auth.set_access_token(access_token,access_token_secret)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.chdir(path)\n",
        "#path = \"\"\n",
        "\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\n",
        "def get_embedding(\n",
        "    text: str, engine=\"text-similarity-davinci-001\"\n",
        ") -> List[float]:\n",
        "   \n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\n",
        "        \"embedding\"\n",
        "    ]\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "\n",
        "def get_df_with_chunks_embedded(text: str) -> pd.DataFrame:\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    #for sentence in sentences:\n",
        "      #print(len(sentence))\n",
        "    chunk_size = 2\n",
        "    #print(chunk_size)\n",
        "    \n",
        "    chunks = [\n",
        "        sentences[i : i + chunk_size]\n",
        "        for i in range(0, len(sentences), chunk_size)\n",
        "    ]\n",
        "\n",
        "    chunks = [\" \".join(chunk) for chunk in chunks]\n",
        "    df_with_chunks = pd.DataFrame(chunks, columns=[\"chunk\"])\n",
        "    df_with_chunks[\"search\"] = df_with_chunks.chunk.apply(\n",
        "        lambda x: get_embedding((x), engine=(\"text-search-davinci-doc-001\"))\n",
        "    )\n",
        "\n",
        "    return df_with_chunks\n",
        "\n",
        "\n",
        "def search_material(df: pd.DataFrame, query: str, n=3) -> pd.DataFrame:\n",
        "    embedding = get_embedding(query, engine=\"text-search-davinci-query-001\")\n",
        "\n",
        "    df[\"similarities\"] = df.search.apply(\n",
        "        lambda x: cosine_similarity(x, embedding)\n",
        "    )\n",
        "\n",
        "    res = df.sort_values(\"similarities\", ascending=False).head(n)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def join_chunks(chunks) -> str:\n",
        "    return \"\\n\".join(chunks)\n",
        "\n",
        "def generate_answer_from_question(info: str, q: str) -> str:\n",
        " \n",
        "    response = completion_with_backoff(\n",
        "        model=\"text-davinci-002\",\n",
        "        \n",
        "        prompt=f\"Only using the information and facts provided below \"\n",
        "        f\"provide a comprehensive and truthful answer to the following question in the form of bullet points.If the answer can't be extracted using below information, say I don't know \\n\\n{info}\\n\\nQuestion:{q}\\n###\\n\\nAnswer:\",\n",
        "        temperature=0.3,\n",
        "        max_tokens=750,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "\n",
        "def get_answer(question: str, df: pd.DataFrame) -> list:\n",
        "    \n",
        "    \n",
        "    question=question.replace(\"-\", \" \").strip()\n",
        "    res = search_material(df, question)\n",
        "    blurb = join_chunks(res[\"chunk\"])\n",
        "    \n",
        "    answer= generate_answer_from_question(blurb, question)\n",
        "       \n",
        "    return answer\n",
        "\n",
        "\n",
        "def generate_illustration(prompt: str, size: str = \"256x256\") -> str:\n",
        "    response = image_gen_with_backoff(\n",
        "        prompt=f\"{prompt}, watercolor illustration\", n=1, size=size\n",
        "    )\n",
        "    return response[\"data\"][0][\"url\"]\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n",
        "def completion_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n",
        "def moderation_with_backoff(**kwargs):\n",
        "    return openai.Moderation.create(**kwargs)\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n",
        "def image_gen_with_backoff(**kwargs):\n",
        "    return openai.Image.create(**kwargs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aqIYu1A_zzC",
        "outputId": "bd3bb503-4583-4dc3-dd57-7d568d42d8a6"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "ovYMcss6-9wH"
      },
      "outputs": [],
      "source": [
        "#Function which answers a given query using  information from twitter handle of a particular user name\n",
        "def get_info(api,user_name,query):\n",
        "  pages = tweepy.Cursor(api.user_timeline,screen_name=user_name).pages(10)\n",
        "  txt=\"\"\n",
        "  for page in (pages):\n",
        "    for i,status in enumerate(page):\n",
        "      txt+=(\" \"+status.text)\n",
        "  chunks=get_df_with_chunks_embedded(txt)\n",
        "  answer=get_answer(query,chunks)\n",
        "  print(f\"{answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "aGMJHXzMxSoq"
      },
      "outputs": [],
      "source": [
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_name1 = 'wackerchemie'   #Change the user name here to extract data from other profile."
      ],
      "metadata": {
        "id": "M0eiyhGyinUQ"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the queries being asked. Change it to ask a different question.\n",
        "query1 = \"Has the company decreased their production capacities in recent times? Answer step by step\"\n",
        "query2 = \"What product does the company sell to the solar panel industry? Answer step by step\""
      ],
      "metadata": {
        "id": "ws2xzRsq8Rbe"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_info(api,user_name1,query1)  #Calling this function provides answer to the query asked "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TpNAIz1976M",
        "outputId": "b475c609-793b-4a23-f14b-994ea5fd2742"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-The company has not decreased their production capacities in recent times.\n",
            "\n",
            "-In fact, the company has been expanding their production capacities in recent times.\n",
            "\n",
            "-The company has plans to expand their silicon metal production in Norway.\n",
            "\n",
            "-The company has plans to increase their global production capacity for silicon rubber.\n",
            "\n",
            "-The company has plans to expand their Charleston site and prepare for construction of silicone production facilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_info(api,user_name1,query2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-exfmswZF3L",
        "outputId": "4394f0f9-c37f-4bd5-a1ab-776d3509dcce"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The company sells HELISOL, a silicone fluid, to the solar panel industry. This fluid is used to harness the sunlight in concentrated solar power (CSP) plants.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#user_name2=\"pvmagazine\"\n",
        "#query3=\"what are some incidents  which impacted the solar panel production. Think step by step\""
      ],
      "metadata": {
        "id": "J5_XX6pwkIHH"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_info(api,user_name2,query3)"
      ],
      "metadata": {
        "id": "QzvG9nv9kYCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}